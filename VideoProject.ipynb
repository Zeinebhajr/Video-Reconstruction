{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQ7rE0mlBxOIcmOczrncK0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zeinebhajr/Video-Reconstruction/blob/main/VideoProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation des librairies"
      ],
      "metadata": {
        "id": "fiOZYtUaR01W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cloner le répo SuperGlue"
      ],
      "metadata": {
        "id": "i4x_N2j0R3Z5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbkUmha3RYMs",
        "outputId": "f7e6c617-af76-4734-8e6d-c1cd8e682bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SuperGluePretrainedNetwork'...\n",
            "remote: Enumerating objects: 185, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 185 (delta 0), reused 2 (delta 0), pack-reused 182 (from 1)\u001b[K\n",
            "Receiving objects: 100% (185/185), 118.85 MiB | 15.22 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/magicleap/SuperGluePretrainedNetwork.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/SuperGluePretrainedNetwork/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ABRBGMZSAxC",
        "outputId": "2e7ebdd8-3a42-4638-c044-565bf3be0020"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/SuperGluePretrainedNetwork/requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/SuperGluePretrainedNetwork/requirements.txt (line 2)) (2.5.1+cu121)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 3.4.11.39, 3.4.17.61, 4.4.0.42, 4.4.0.44, 4.5.4.58, 4.5.5.62, 4.7.0.68\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.1.2.30 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.60, 4.5.5.64, 4.6.0.66, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80, 4.10.0.82, 4.10.0.84)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==4.1.2.30\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /content/SuperGluePretrainedNetwork/models/ https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/models/weights/superglue_indoor.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0OfQ2coSCjl",
        "outputId": "757d67d5-6c58-46f9-fbb4-56ec41dac1c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-17 16:20:24--  https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/models/weights/superglue_indoor.pth\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘/content/SuperGluePretrainedNetwork/models/superglue_indoor.pth’\n",
            "\n",
            "superglue_indoor.pt     [ <=>                ] 165.57K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-12-17 16:20:24 (15.0 MB/s) - ‘/content/SuperGluePretrainedNetwork/models/superglue_indoor.pth’ saved [169544]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/SuperGluePretrainedNetwork')"
      ],
      "metadata": {
        "id": "fjZhQcb0Sp9C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importer les librairies"
      ],
      "metadata": {
        "id": "4rBqoHBqSu09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMa5VZSFVY0Z",
        "outputId": "a43b8bad-be6d-486c-a6c1-4ac6f659cc31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.7.4-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting kornia-rs>=0.1.0 (from kornia)\n",
            "  Downloading kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (24.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.1->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (3.0.2)\n",
            "Downloading kornia-0.7.4-py2.py3-none-any.whl (899 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.4/899.4 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kornia-rs, kornia\n",
            "Successfully installed kornia-0.7.4 kornia-rs-0.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from models.matching import Matching  # Assuming you have the Matching class correctly defined\n",
        "import kornia as K"
      ],
      "metadata": {
        "id": "HNRxsMPSS1IQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Elimination des images outliers"
      ],
      "metadata": {
        "id": "JEPIq6E_S7vg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resnet"
      ],
      "metadata": {
        "id": "WgJ7YjNzTDnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraction des features des images grace à Resnet"
      ],
      "metadata": {
        "id": "T8WUNmTqTFbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frame_embeddings(video_path, model, transform, device=\"gpu\"):\n",
        "    \"\"\"\n",
        "    Extract frame embeddings using a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the video file.\n",
        "        model (torch.nn.Module): Pre-trained PyTorch model.\n",
        "        transform (torchvision.transforms.Compose): Transformations for model input.\n",
        "        device (str): Device for computation ('cpu' or 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        embeddings (list): List of frame embeddings.\n",
        "        frame_indices (list): List of frame indices corresponding to embeddings.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    embeddings = []\n",
        "    frame_indices = []\n",
        "    frame_idx = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Resize and normalize the frame\n",
        "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (224, 224))\n",
        "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        # Extract embedding\n",
        "        with torch.no_grad():\n",
        "            embedding = model(img_tensor).squeeze().cpu().numpy()\n",
        "            embeddings.append(embedding)\n",
        "            frame_indices.append(frame_idx)\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    return embeddings, frame_indices"
      ],
      "metadata": {
        "id": "bdyhbrnATCO0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification du type d'image avec DBSCAN"
      ],
      "metadata": {
        "id": "VCqjl8AvTQ3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_outliers_with_dbscan(embeddings, eps=0.5, min_samples=5):\n",
        "    \"\"\"\n",
        "    Detect outlier frames using DBSCAN clustering.\n",
        "\n",
        "    Args:\n",
        "        embeddings (list): List of frame embeddings.\n",
        "        eps (float): Maximum distance between points in a cluster.\n",
        "        min_samples (int): Minimum number of points in a cluster.\n",
        "\n",
        "    Returns:\n",
        "        outlier_indices (list): Indices of outlier frames.\n",
        "    \"\"\"\n",
        "    embeddings = np.array(embeddings)\n",
        "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(embeddings)\n",
        "    labels = clustering.labels_\n",
        "\n",
        "    # Outliers are labeled as -1 in DBSCAN\n",
        "    outlier_indices = [i for i, label in enumerate(labels) if label == -1]\n",
        "    return outlier_indices"
      ],
      "metadata": {
        "id": "9G4n9VPPTQLx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtention de la video sans outliers"
      ],
      "metadata": {
        "id": "w1akuQgATkgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/corrupted_video.mp4\""
      ],
      "metadata": {
        "id": "wBwhBsw1UHyI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
        "model = model.eval().to(device)\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "embeddings, frame_indices = extract_frame_embeddings(video_path, model, transform, device)\n",
        "outlier_indices = detect_outliers_with_dbscan(embeddings, eps=5.5, min_samples=2)\n",
        "cap=cv2.VideoCapture(video_path)\n",
        "height, width= 1080 , 1920\n",
        "out = cv2.VideoWriter(\"video_pure.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 25, (width, height))\n",
        "i=0\n",
        "while cap.isOpened():\n",
        "  ret,frame=cap.read()\n",
        "  if not ret:\n",
        "    break\n",
        "  if i not in outlier_indices:\n",
        "    out.write(frame)\n",
        "  i+=1\n",
        "out.release()\n",
        "vid='video_pure.mp4'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQeB4x0mTpi8",
        "outputId": "ccb067ea-a281-417c-f621-4b714e450878"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 131MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reordonnecement des frames"
      ],
      "metadata": {
        "id": "QFceQPK4UN1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrice de similitudes des features  globaux\n"
      ],
      "metadata": {
        "id": "uS-UFFYYUb6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity_matrix(embeddings):\n",
        "    \"\"\"Compute pairwise cosine similarity for embeddings.\"\"\"\n",
        "    return cosine_similarity(embeddings)\n"
      ],
      "metadata": {
        "id": "TzMom_o6U8gA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrice de similarités des points d'interets"
      ],
      "metadata": {
        "id": "UUd_kzyIU9VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class CorrespondenceExtractor:\n",
        "    def __init__(self, device, resize=640, descriptor_size=256, weight_similarity=0.5):\n",
        "        self.device = device\n",
        "        self.resize = resize\n",
        "        self.descriptor_size = descriptor_size  # Fixed descriptor size\n",
        "        self.weight_similarity = weight_similarity  # Weight for combining similarity matrices\n",
        "\n",
        "        # Initialize SuperPoint and SuperGlue parameters\n",
        "        superpoint_params = {\n",
        "            'nms_radius': 4,\n",
        "            'keypoint_threshold': 0.005,\n",
        "            'max_keypoints': 1024\n",
        "        }\n",
        "        superglue_params = {\n",
        "            'weights': 'indoor',\n",
        "            'sinkhorn_iterations': 20,\n",
        "            'match_threshold': 0.2\n",
        "        }\n",
        "\n",
        "        # Instantiate the SuperGlue model using the Matching class\n",
        "        self.matching_model = Matching({\n",
        "            'superpoint': superpoint_params,\n",
        "            'superglue': superglue_params\n",
        "        }).eval().to(self.device)\n",
        "\n",
        "    def extract_correspondences(self, prev_frame, curr_frame):\n",
        "        \"\"\"Extract corresponding keypoints and descriptors between two frames.\"\"\"\n",
        "        prev_tensor = self._preprocess_frame(prev_frame)\n",
        "        curr_tensor = self._preprocess_frame(curr_frame)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = self.matching_model({\n",
        "                'image0': prev_tensor,\n",
        "                'image1': curr_tensor\n",
        "            })\n",
        "\n",
        "        keypoints0, keypoints1, matches, scores = self._extract_matches(predictions)\n",
        "        descriptors0 = predictions['descriptors0'][0].cpu().numpy()\n",
        "        descriptors1 = predictions['descriptors1'][0].cpu().numpy()\n",
        "\n",
        "        return keypoints0, keypoints1, matches, scores, descriptors0, descriptors1\n",
        "\n",
        "    def _preprocess_frame(self, frame):\n",
        "        \"\"\"Convert frame to grayscale, resize, and normalize for SuperGlue.\"\"\"\n",
        "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        tensor = torch.from_numpy(gray_frame).unsqueeze(0).unsqueeze(0).float().to(self.device) / 255.0\n",
        "        resize_transform = K.augmentation.Resize(self.resize, side=\"long\")\n",
        "        return resize_transform(tensor)\n",
        "\n",
        "    def _extract_matches(self, predictions):\n",
        "        \"\"\"Extract keypoints, matches, and scores from SuperGlue predictions.\"\"\"\n",
        "        keypoints0 = predictions['keypoints0'][0].cpu().numpy()\n",
        "        keypoints1 = predictions['keypoints1'][0].cpu().numpy()\n",
        "        matches = predictions['matches0'][0].cpu().numpy()\n",
        "        scores = predictions['matching_scores0'][0].cpu().numpy()\n",
        "        valid = matches > -1\n",
        "        matched_keypoints0 = keypoints0[valid]\n",
        "        matched_keypoints1 = keypoints1[matches[valid]]\n",
        "        return matched_keypoints0, matched_keypoints1, matches, scores\n",
        "\n",
        "# Function to compute the similarity matrix based on the number of matches\n",
        "def compute_match_similarity(video_frames, correspondence_extractor):\n",
        "    \"\"\"Compute similarity based on the number of matches using SuperGlue.\"\"\"\n",
        "    n_frames = len(video_frames)\n",
        "    match_similarity_matrix = np.zeros((n_frames, n_frames))\n",
        "\n",
        "    for i in range(n_frames):\n",
        "        for j in range(i + 1, n_frames):\n",
        "            frame1 = video_frames[i]\n",
        "            frame2 = video_frames[j]\n",
        "\n",
        "            # Extract correspondences using CorrespondenceExtractor\n",
        "            keypoints0, keypoints1, matches, scores, descriptors0, descriptors1 = correspondence_extractor.extract_correspondences(frame1, frame2)\n",
        "\n",
        "            # Count number of valid matches\n",
        "            num_matches = np.sum(matches > -1)  # Count number of valid matches\n",
        "\n",
        "            # Assign similarity based on the number of matches\n",
        "            similarity = num_matches / len(matches)  # Normalize the similarity\n",
        "            match_similarity_matrix[i, j] = similarity\n",
        "            match_similarity_matrix[j, i] = similarity  # Symmetric matrix\n",
        "\n",
        "    return match_similarity_matrix\n",
        "\n",
        "# Load the video and extract frames\n",
        "def load_video_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    video_frames = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Convert frame from BGR (OpenCV format) to RGB\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        video_frames.append(frame_rgb)\n",
        "    cap.release()\n",
        "\n",
        "    return video_frames\n",
        "\n",
        "# Main function to process the video and compute final similarity matrix\n",
        "def main(video_path, correspondence_extractor):\n",
        "    # Load video frames\n",
        "    video_frames = load_video_frames(video_path)\n",
        "\n",
        "    # Compute similarity matrix based on SuperGlue matches\n",
        "    match_similarity_matrix = compute_match_similarity(video_frames, correspondence_extractor)\n",
        "\n",
        "    return match_similarity_matrix\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    video_path = vid\n",
        "\n",
        "    correspondence_extractor = CorrespondenceExtractor(device='cuda')\n",
        "\n",
        "    # Process the video and compute the similarity matrix\n",
        "    final_similarity_matrix = main(video_path, correspondence_extractor)\n",
        "\n",
        "    print(\"Final similarity matrix:\")\n",
        "    print(final_similarity_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW73wiBoVEGd",
        "outputId": "75f76691-62e0-4416-fd3d-69e7a69f2b66"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/SuperGluePretrainedNetwork/models/superpoint.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(str(path)))\n",
            "/content/SuperGluePretrainedNetwork/models/superglue.py:226: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(str(path)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SuperPoint model\n",
            "Loaded SuperGlue model (\"indoor\" weights)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-12-9b6fcece8589>\", line 122, in <cell line: 116>\n",
            "    final_similarity_matrix = main(video_path, correspondence_extractor)\n",
            "  File \"<ipython-input-12-9b6fcece8589>\", line 111, in main\n",
            "    match_similarity_matrix = compute_match_similarity(video_frames, correspondence_extractor)\n",
            "  File \"<ipython-input-12-9b6fcece8589>\", line 77, in compute_match_similarity\n",
            "    keypoints0, keypoints1, matches, scores, descriptors0, descriptors1 = correspondence_extractor.extract_correspondences(frame1, frame2)\n",
            "  File \"<ipython-input-12-9b6fcece8589>\", line 36, in extract_correspondences\n",
            "    predictions = self.matching_model({\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/SuperGluePretrainedNetwork/models/matching.py\", line 82, in forward\n",
            "    pred = {**pred, **self.superglue(data)}\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/SuperGluePretrainedNetwork/models/superglue.py\", line 253, in forward\n",
            "    desc0, desc1 = self.gnn(desc0, desc1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/SuperGluePretrainedNetwork/models/superglue.py\", line 138, in forward\n",
            "    delta0, delta1 = layer(desc0, src0), layer(desc1, src1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/SuperGluePretrainedNetwork/models/superglue.py\", line 120, in forward\n",
            "    message = self.attn(x, source, source)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/SuperGluePretrainedNetwork/models/superglue.py\", line 109, in forward\n",
            "    return self.merge(x.contiguous().view(batch_dim, self.dim*self.num_heads, -1))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 375, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 370, in _conv_forward\n",
            "    return F.conv1d(\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 868, in getmodule\n",
            "    for modname, module in sys.modules.copy().items():\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "jointure des 2 matrices"
      ],
      "metadata": {
        "id": "EIbo1P_uVK0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def join_similarities(similarity_embeddings,similarity_keypoints,alpha):\n",
        "  return alpha * similarity_keypoints + (1 - alpha) * similarity_embeddings\n"
      ],
      "metadata": {
        "id": "qOVnEhJvVKGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "algo ordonner"
      ],
      "metadata": {
        "id": "AB6NcSqRV5vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_frames_by_similarity(similarity_matrix):\n",
        "    \"\"\"Reorder frames using nearest neighbor chaining based on similarity.\"\"\"\n",
        "    n_frames = similarity_matrix.shape[0]\n",
        "    # Start with the most central frame\n",
        "    central_frame = np.argmax(np.mean(similarity_matrix, axis=1))\n",
        "    ordered_indices = [central_frame]\n",
        "    visited = set(ordered_indices)\n",
        "\n",
        "    while len(ordered_indices) < n_frames:\n",
        "        last_frame = ordered_indices[-1]\n",
        "        # Find the most similar unvisited frame\n",
        "        next_frame = np.argmax(similarity_matrix[last_frame])\n",
        "        while next_frame in visited:\n",
        "            similarity_matrix[last_frame, next_frame] = -1  # Mark as visited\n",
        "            next_frame = np.argmax(similarity_matrix[last_frame])\n",
        "        ordered_indices.append(next_frame)\n",
        "        visited.add(next_frame)\n",
        "\n",
        "    return ordered_indices\n"
      ],
      "metadata": {
        "id": "SdO8NbMyV3f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "enregistrer la video"
      ],
      "metadata": {
        "id": "5R5X42bwV-I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_reordered_video(video_path, frame_order, output_path):\n",
        "    \"\"\"Save reordered video based on frame order.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    print(len(frames))\n",
        "    # Reorder frames\n",
        "    reordered_frames = [frames[i] for i in frame_order]\n",
        "\n",
        "    # Save reordered video\n",
        "    height, width, layers = reordered_frames[0].shape\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 25, (width, height))\n",
        "    for frame in reordered_frames:\n",
        "        out.write(frame)\n",
        "    out.release()"
      ],
      "metadata": {
        "id": "mAGnevDDV_1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "finalement"
      ],
      "metadata": {
        "id": "v6RNlLJeWC7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combinaison des matrices de similarité :**\n",
        "La nouvelle version utilise une combinaison linéaire pondérée entre la matrice de similarité des embeddings (ResNet) et celle des keypoints (SuperGlue). La combinaison est définie comme :\n",
        "\n",
        "*similarity_combined\n",
        "=\n",
        "𝛼\n",
        "×\n",
        "similarity_embeddings\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛼\n",
        ")\n",
        "×\n",
        "similarity_keypoints*\n",
        "\n",
        "\n",
        "*similarity_combined=α×similarity_embeddings+(1−α)×similarity_keypoints\n",
        "où\n",
        "𝛼\n",
        "∈\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "α∈[0,1].*\n",
        "\n",
        "**Robustesse face aux incohérences** :\n",
        "\n",
        "Si alpha = 1, seule la matrice des embeddings est utilisée (résultat homogène mais incohérences temporelles possibles).\n",
        "Si alpha = 0, seule la matrice des keypoints est utilisée (cohérence temporelle mais risque de \"lags\").\n",
        "Vous pouvez expérimenter différentes valeurs de\n",
        "𝛼\n",
        "α pour trouver un équilibre adapté à votre vidéo."
      ],
      "metadata": {
        "id": "4bwdmi5AXCT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings, frame_indices = extract_frame_embeddings(vid, model, transform, device)\n",
        "normalized_embeddings = normalize(embeddings)\n",
        "similarity_embeddings = compute_similarity_matrix(embeddings)\n",
        "matrix=np.eye(final_similarity_matrix.shape[0])-final_similarity_matrix\n",
        "sim_totale=join_similarities(similarity_embeddings,matrix,0.9)\n",
        "ordered_indices = reorder_frames_by_similarity(sim_totale)\n",
        "save_reordered_video(vid, ordered_indices, 'reordered_video.mp4')"
      ],
      "metadata": {
        "id": "j7ZF-EeeWCSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}